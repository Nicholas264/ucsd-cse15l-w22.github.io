--
layout: page-fullwidth
subheadline: Week 1
published: true
title:  "Week 4 – When Tests Accumulate"
tags:
    - post week
categories:
    - week
header: no
todos:
    - due-date: "11:59am January 26, 2021"
      name: "Quiz 4"
      url: "https://www.gradescope.com"
current: true
---


## Reading and Links

_Coming soon_

## Notes from Class

_Coming soon_

## Lab Tasks

Clone (or pull, if it's your repository) the repository with your group's code
from last lab. You should have the original provided test file `test-file.md`,
and three other test files that you wrote as part of the last lab. If your group
doesn't have this many test files, create one more, then commit and push it.

**Meta-comment**: One thing that is common in programming but _uncommon_ in many
programming courses is revisiting the same program for many weeks in a row. We
are going to work with this markdown parser for many weeks. It will help us
understand how code evolves over time and how a repository helps us track that
evolution.

### Your Memory

Run the program on one of the examples you wrote last week. Is the output
correct? How do you know?

**Write in notes** what process did your team go through to justify that the
output was correct? Did you remember what it was supposed to be or did you have
to open the file to verify?

### Running via the Command Line

Have someone share screen with VScode open on the markdown parser project with
their terminal open. They should make a small edit to the program (like adding a
print statement in `main`).

Start a timer, then have them recompile the program and run the program on _all_
of the test files (there should be 4 total), then stop the timer.

**Write in notes**: How long did this take?

Note that this time doesn't include any time you'd have to spend reading output
to check that it's correct, etc, because we're pretty sure things are working
from our process in lab 3. That's a lot of time to spend on each change,
checking that we haven't broken the behavior for an existing test!

### Testing Tools

These two issues—remembering what the correct result ought to be, and the work
involved in re-running tests one-by-one—motivate using _automated testing
tools_. There are lots of choices we could make here. We're going to start by
using JUnit not least because it is representative of many similar tools, and
sees widespread use in large software projects.

For this part of the lab, you'll install JUnit and use it to write a test
program that solves the problems of having to remember what “correct output” is
and taking a lot of manual effort to re-run many tests.

### Setting up JUnit

Download these two `.jar` files:

- junit4 jar link
- hamcrest jar link

Then, make a directory called `lib` in your project, and copy both of those
files to that directory. Commit and push the files once they are added to `lib`
(this is a useful step because it ensures that you see them in your repository!)

Next, create a file called `MarkdownParseTest.java` in your repository. Put the
following code in the file:

```
import static org.junit.Assert.*;
import org.junit.*;

public class MarkdownParseTest {
    @Test
    public void addition() {
        assertEquals(2, 1 + 1);
    }
}
```

Then, run this code at the command line using these two commands:

```
javac -cp .:lib/junit-4.13.2.jar:lib/hamcrest-core-1.3.jar MarkdownParseTest.java

java -cp .:lib/junit-4.13.2.jar:lib/hamcrest-core-1.3.jar org.junit.runner.JUnitCore MarkdownParseTest
```


**Write in notes**: Copy the output of this command, including any errors if you
didn't use it correctly the first time.

**Write in notes**: Copy the test file code into the notes. On each line,
describe what you think that line is doing. If you aren't sure, write it down as
a question.

**Post on Piazza**: Take all the open questions you have that you couldn't
answer with your group and tutor, and post them as public questions on Piazza.
Sign the post with your team animal name.

**Commit to Github**: Once you get this command working, add the test file and
commit and push it to Github. For your commit message, put the commands you used
to run the tests so you'll have a record of what worked later.

### Writing our Tests with JUnit

The example above just tests that `1+1` is `==` to `2` from a JUnit tutorial.
Next, we should add another test to actually do some work with our code.

Your task is to add another test method to `MarkdownParseTest` that:

- Calls `getLinks` on the contents of `test-file.md`
- Checks that the result is equal to a list containing `https://something.com` and `some-page.html`

**Hints**: Remember that `Files.readString` and `Path.of` are useful for reading
the contents of files, and require using `throws` clauses. You can use
`List.of("a", "b", "c")` to easily create a `List` of strings.

**Write in notes**: Put at least one error you encountered during this process
into the doc as a screenshot.

When you have the test successfully passing, add and commit the changes, then
push to Github.

Then, add separate test methods to test _each_ of the test files you've written
with `getLinks`. Make sure all the tests pass, and commit and push to Github.

### The Benefits of Automated Tests

Have someone ready at the terminal in VScode with the project open. Start a
timer. Have them run the commands to compile and run the tests. Stop the timer.

**Write down in notes** How much quicker was it to run the JUnit tests than to
run the test for each file individually?


## Lab Report

_Coming soon_

For this lab report, you will work with the terminology of _bug_, _symptom_, and
_failure-inducing input_ from Rehger, along with the work you did in the week 3
lab.

Your report should have 3 sections.

- Section 1, relating to the _original code_ for `MarkdownParse` provided with lab 3
- Section 2, relating to the _code after your first fix_ for `MarkdownParse`, committed as part of lab 3
- Section 2, relating to the _code after your second fix_ for `MarkdownParse`, committed as part of lab 3

For each section, take the code for `MarkdownParse.java` in the corresponding
commit from your lab report. Compile and run it on _two different test files_
you created as part of the lab.

For each run, indicate:

- If each file is a _failure-inducing input_ or not
- What the _symptom_ is for each one that is failure-inducing, and if the
symptom is different from the others
- What the corresponding _bug_ was for that symptom

Summarize the results in a table; for example:

| Version | your-test1 | your-test2 |
|-----|-----|----|----|
| Original | Symptom: infinite loop | Symptom: infinite loop |
| After 1st fix | Not failure-inducing | Symptom: too many links printed | Sympt
| After 2nd fix |
